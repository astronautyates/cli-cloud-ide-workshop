## Astronomer DAG authoring workshop

Getting started with Airflow, the de facto standard for data orchestration with over 12M downloads per month, has never been easier. With a combination of new Airflow features, open source community projects, and Astro products, you can create complex data pipelines that leverage Airflow’s best-in-class orchestration while writing less code and without extensive Airflow knowledge.

This repo supports a public workshop where we’ll take you from running Airflow locally, to writing your first DAG, to implementing ELT use cases with simple and complex datasets. In just a couple of hours, you’ll learn:

- How to run Airflow locally using the open source [Astro CLI](https://docs.astronomer.io/astro/cli/overview) for easy local development.
- How to express complex, dynamic use cases in your DAGs.
- How to write an ELT DAG that implements SQL and Python functions with no boilerplate code using the Astro Python SDK, a new open source SDK designed for rapid development of ETL/ELT workflows.
- How Astronomer is making DAG writing easier, including the [Astro Cloud IDE](https://docs.astronomer.io/astro/cloud-ide), a notebook-inspired way to write data pipelines.

Instructions for Parts 1-3 of the workshop can be found in the `include/workshop-instructions/` directory. Parts 1-2 can be completed locally using only the Astro CLI. Part 3 requires access to the Astro Cloud IDE. 

This workshop is intended to be completed during a live event conducted by Astronomer.
